{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61f56afc-bc15-46a4-8eb1-d940c332cf52",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "And now - this colab unveils the heart (or the brains?) of the transformers library - the models:\n",
    "\n",
    "https://colab.research.google.com/drive/1hhR9Z-yiqjUe7pJjVQw4c74z_V3VchLy?usp=sharing\n",
    "\n",
    "This should run nicely on a low-cost or free T4 box."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbed898",
   "metadata": {},
   "source": [
    "my colab notebook:\n",
    "\n",
    "https://colab.research.google.com/drive/1-dUyawPg7d23opO_mBEwqsKoOEbKU-9j?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b15f53",
   "metadata": {},
   "source": [
    "## Some notes on the code:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff38dd8",
   "metadata": {},
   "source": [
    "```python\n",
    "from transformers import BitsAndBytesConfig\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                            # å¼€å¯ 4-bit åŠ è½½\n",
    "    bnb_4bit_use_double_quant=True,               # ä½¿ç”¨åŒé‡é‡åŒ–\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,        # æ¨ç†æ—¶ä½¿ç”¨ bfloat16\n",
    "    bnb_4bit_quant_type=\"nf4\"                     # ä½¿ç”¨ NF4 é‡åŒ–ç±»å‹\n",
    ")\n",
    "```\n",
    "`BitsAndBytesConfig` æ˜¯ `Transformers` æä¾›çš„ä¸€ä¸ªç±»ï¼Œç”¨äºé…ç½®å’Œæ§åˆ¶é€šè¿‡ `bitsandbytes` åº“å¯¹æ¨¡å‹è¿›è¡Œé‡åŒ–åŠ è½½çš„æ–¹å¼ï¼Œå¸¸ç”¨äºèŠ‚çœæ˜¾å­˜ã€åŠ é€Ÿæ¨ç†ã€‚\n",
    "\n",
    "ğŸ”¹`load_in_4bit=True`\n",
    "\n",
    "- è¡¨ç¤ºå°†æ¨¡å‹åŠ è½½ä¸º 4-bit é‡åŒ–æ ¼å¼ï¼ˆè€Œä¸æ˜¯å¸¸è§çš„ FP16ã€INT8 ç­‰ï¼‰ï¼Œæå¤§åœ°å‡å°‘æ˜¾å­˜ä½¿ç”¨ï¼ˆæ¯”å¦‚ä¸€ä¸ª 13B æ¨¡å‹èƒ½åœ¨ 24GB GPU ä¸Šè¿è¡Œï¼‰ã€‚\n",
    "\n",
    "- éœ€ä¾èµ– `bitsandbytes` åº“ã€‚\n",
    "\n",
    "ğŸ”¹ `bnb_4bit_use_double_quant=True`\n",
    "\n",
    "- å¯ç”¨ åŒé‡é‡åŒ–ï¼ˆDouble Quantizationï¼‰ï¼š\n",
    "\n",
    "- å…ˆå°†æƒé‡é‡åŒ–ä¸ºæ›´ä½ç²¾åº¦ï¼Œå†å¯¹é‡åŒ–å™¨æœ¬èº«ä¹Ÿè¿›è¡Œå‹ç¼©ã€‚\n",
    "\n",
    "- è¿›ä¸€æ­¥å‡å°‘æ¨¡å‹ä½“ç§¯ï¼Œä½†ä¼šç¨å¾®å¢åŠ è®¡ç®—å¤æ‚åº¦ã€‚\n",
    "\n",
    "- åœ¨å¤š GPU éƒ¨ç½²ä¸‹å¯æ˜¾è‘—èŠ‚çœå†…å­˜ã€‚\n",
    "\n",
    "ğŸ”¹ `bnb_4bit_compute_dtype=torch.bfloat16`\n",
    "\n",
    "- æŒ‡å®šæ¨¡å‹æ¨ç†æ—¶çš„è®¡ç®—ç²¾åº¦ã€‚\n",
    "\n",
    "- `torch.bfloat16`ï¼ˆBrain Float 16ï¼‰åœ¨æ–°ä¸€ä»£ GPUï¼ˆå¦‚ A100ã€H100ã€4090ï¼‰ä¸Šå…·æœ‰è‰¯å¥½æ”¯æŒï¼Œä¿ç•™åŠ¨æ€èŒƒå›´çš„åŒæ—¶èŠ‚çœæ˜¾å­˜ã€‚\n",
    "\n",
    "- å¯æ›¿ä»£ torch.float16ï¼ˆæ ‡å‡†åŠç²¾åº¦ï¼‰ï¼Œä½†å¯¹ç²¾åº¦æ›´å‹å¥½ã€‚\n",
    "\n",
    "ğŸ”¹ `bnb_4bit_quant_type=\"nf4\"`\n",
    "\n",
    "- é€‰æ‹©é‡åŒ–æ–¹æ³•ä¸º NF4ï¼ˆNormal Float 4ï¼‰ï¼š\n",
    "\n",
    "- æ˜¯ `bitsandbytes` æä¾›çš„ä¸€ç§é«˜çº§ 4-bit é‡åŒ–æ–¹æ³•ï¼Œç›¸æ¯”ä¼ ç»Ÿ fp4 æ›´å‡†ç¡®ã€‚\n",
    "\n",
    "- å…·æœ‰æ›´å¥½çš„ä¿çœŸåº¦ï¼ˆå¯¹å¤§æ¨¡å‹æ•ˆæœå½±å“æ›´å°ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa86756",
   "metadata": {},
   "source": [
    "è¿™ç§è®¾ç½®éå¸¸é€‚åˆï¼š\n",
    "\n",
    "- åŠ è½½å¤§è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ LLaMA-2ã€Mistralã€Baichuan ç­‰ï¼‰åˆ°æ˜¾å­˜æœ‰é™çš„ GPUï¼ˆ24GB æˆ–æ›´å°‘ï¼‰ã€‚\n",
    "\n",
    "- èŠ‚çœå†…å­˜åŒæ—¶ä¿æŒè¾ƒé«˜æ¨ç†æ€§èƒ½ã€‚\n",
    "\n",
    "- é€‚åˆæ¨ç†å’Œè½»é‡çº§å¾®è°ƒï¼ˆLoRAï¼‰ä»»åŠ¡ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3871cbb",
   "metadata": {},
   "source": [
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "def generate(model, messages):\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "  tokenizer.pad_token = tokenizer.eos_token\n",
    "  inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
    "  streamer = TextStreamer(tokenizer)\n",
    "  model = AutoModelForCausalLM.from_pretrained(model, device_map=\"auto\", quantization_config=quant_config)\n",
    "  outputs = model.generate(inputs, max_new_tokens=80, streamer=streamer)\n",
    "  del model, inputs, tokenizer, outputs, streamer\n",
    "  gc.collect()\n",
    "  torch.cuda.empty_cache()\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f76f68",
   "metadata": {},
   "source": [
    "ğŸ”¹ åŠ è½½åˆ†è¯å™¨ï¼ˆTokenizerï¼‰\n",
    "```python\n",
    "tokenizer = AutoTokenizer.from_pretrained(model) # æ ¹æ®æ¨¡å‹åè‡ªåŠ¨åŠ è½½ç›¸åº”çš„åˆ†è¯å™¨ã€‚\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "```\n",
    "`tokenizer.pad_token = tokenizer.eos_token`ï¼šå°† \\<pad\\> token è®¾ç½®ä¸º \\<eos\\>ï¼ˆç»“æŸç¬¦ï¼‰ã€‚ä¸€äº›æ¨¡å‹æ²¡æœ‰æ˜¾å¼çš„ pad tokenï¼Œè¿™æ ·å¯ä»¥é¿å…é”™è¯¯ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fca0846",
   "metadata": {},
   "source": [
    "ğŸ”¹ åº”ç”¨èŠå¤©æ¨¡æ¿å¹¶è½¬ä¸ºå¼ é‡\n",
    "```python\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    return_tensors=\"pt\",\n",
    "    add_generation_prompt=True\n",
    ").to(\"cuda\")\n",
    "```\n",
    "`apply_chat_template`ï¼šå°†å¯¹è¯ messages è½¬æ¢ä¸ºæ¨¡å‹æ‰€éœ€çš„æ ¼å¼ï¼ˆå¦‚ ChatMLã€Llama 2 çš„ prompt æ ¼å¼ç­‰ï¼‰ã€‚\n",
    "\n",
    "`return_tensors=\"pt\"`ï¼šå°†å¤„ç†åçš„æ–‡æœ¬è½¬æ¢ä¸º PyTorch å¼ é‡ã€‚\n",
    "\n",
    "`add_generation_prompt=True`ï¼šä¸ºç”Ÿæˆæ·»åŠ ä¸€ä¸ªâ€œæç¤ºä½â€ï¼ˆè®©æ¨¡å‹çŸ¥é“è¦ç»§ç»­ç”Ÿæˆå›å¤ï¼‰ã€‚\n",
    "\n",
    "`.to(\"cuda\")`ï¼šå°†è¾“å…¥æ•°æ®ç§»åŠ¨åˆ° GPUã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eada6e",
   "metadata": {},
   "source": [
    "ğŸ”¹ åˆ›å»ºæµå¼è¾“å‡ºå™¨\n",
    "```python\n",
    "streamer = TextStreamer(tokenizer)\n",
    "```\n",
    "`TextStreamer` æ˜¯ä¸€ä¸ªç”¨äºå®æ—¶è¾“å‡ºç”Ÿæˆæ–‡æœ¬çš„å·¥å…·ï¼Œå®ƒä¼šåœ¨æ¨¡å‹ç”Ÿæˆçš„è¿‡ç¨‹ä¸­å®æ—¶æ‰“å°ç»“æœã€‚\n",
    "ï¼ˆé€šå¸¸æ˜¯ Hugging Face çš„ `transformers.TextStreamer`ã€‚ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa129a1",
   "metadata": {},
   "source": [
    "ğŸ”¹ åŠ è½½æ¨¡å‹\n",
    "```python\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quant_config\n",
    ")\n",
    "```\n",
    "`AutoModelForCausalLM.from_pretrained(...)`ï¼šåŠ è½½ä¸€ä¸ªè‡ªå›å½’è¯­è¨€æ¨¡å‹ï¼ˆCausal LMï¼‰ã€‚\n",
    "\n",
    "`device_map=\"auto\"`ï¼šè‡ªåŠ¨å°†æ¨¡å‹åˆ†å¸ƒåˆ°å¯ç”¨çš„ GPU ä¸Šã€‚\n",
    "\n",
    "`quantization_config=quant_config`ï¼šä½¿ç”¨é‡åŒ–é…ç½®åŠ è½½æ¨¡å‹ï¼ˆä¾‹å¦‚ 4-bitã€8-bit ä»¥èŠ‚çœæ˜¾å­˜ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42650afc",
   "metadata": {},
   "source": [
    "ğŸ”¹ æ–‡æœ¬ç”Ÿæˆ\n",
    "```python\n",
    "outputs = model.generate(inputs, max_new_tokens=80, streamer=streamer)\n",
    "```\n",
    "`model.generate(...)`ï¼šç”Ÿæˆæœ€å¤š 80 ä¸ªæ–° token çš„å›å¤ã€‚\n",
    "\n",
    "`streamer=streamer`ï¼šè¾¹ç”Ÿæˆè¾¹æ‰“å°ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d08406",
   "metadata": {},
   "source": [
    "ğŸ”¹ æ¸…ç†å†…å­˜\n",
    "```python\n",
    "del model, inputs, tokenizer, outputs, streamer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "```\n",
    "åˆ é™¤å˜é‡ï¼Œé‡Šæ”¾æ˜¾å­˜ã€‚\n",
    "\n",
    "`gc.collect()`ï¼šè§¦å‘ Python çš„åƒåœ¾å›æ”¶æœºåˆ¶ã€‚\n",
    "\n",
    "`torch.cuda.empty_cache()`ï¼šæ¸…ç† CUDA ç¼“å­˜ï¼Œé˜²æ­¢æ˜¾å­˜å ç”¨æŒç»­ä¸Šå‡ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90f220a",
   "metadata": {},
   "source": [
    "## è§£é‡ŠLlamaForCausalLMçš„ç»“æ„"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab1a23d",
   "metadata": {},
   "source": [
    "è¿™æ®µæ¨¡å‹ç»“æ„æ˜¯ä¸€ä¸ª **LLaMA å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”¨äºè‡ªå›å½’è¯­è¨€å»ºæ¨¡ï¼ˆCausal LMï¼‰** çš„ç²¾ç®€è¡¨ç¤ºï¼Œæ¨¡å‹ä¸º 4-bit é‡åŒ–ç‰ˆæœ¬ã€‚æˆ‘ä»¬é€å±‚è§£é‡Šè¿™ä»½æ¶æ„ï¼š\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”§ é¡¶å±‚æ¨¡å‹\n",
    "\n",
    "```python\n",
    "LlamaForCausalLM(\n",
    "  (model): LlamaModel(...)\n",
    "  (lm_head): Linear(...)\n",
    ")\n",
    "```\n",
    "\n",
    "è¿™æ˜¯ Hugging Face `transformers` ä¸­çš„æ ‡å‡†å°è£…ï¼š\n",
    "\n",
    "* `LlamaForCausalLM`ï¼šç”¨äºè¯­è¨€å»ºæ¨¡ï¼ˆå¦‚ç”Ÿæˆã€è¡¥å…¨ï¼‰ï¼Œç»§æ‰¿è‡ª `PreTrainedModel`ã€‚\n",
    "* `model`: å®é™…çš„ Transformer ä¸»ä½“ï¼ˆ`LlamaModel`ï¼‰ã€‚\n",
    "* `lm_head`: è¯­è¨€å»ºæ¨¡å¤´éƒ¨ï¼ˆå°†éšè—å±‚æ˜ å°„åˆ°è¯è¡¨çš„çº¿æ€§å±‚ï¼‰ï¼Œç”¨äºé¢„æµ‹ä¸‹ä¸€ä¸ª tokenã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§± åµŒå…¥å±‚\n",
    "\n",
    "```python\n",
    "(embed_tokens): Embedding(128256, 4096)\n",
    "```\n",
    "\n",
    "* `Embedding(vocab_size=128256, hidden_size=4096)`ï¼š\n",
    "\n",
    "  * å°†è¾“å…¥ token ID æ˜ å°„ä¸ºç»´åº¦ä¸º 4096 çš„å‘é‡ã€‚\n",
    "  * `128256` æ˜¯è¯è¡¨å¤§å°ã€‚\n",
    "  * `4096` æ˜¯æ¨¡å‹çš„éšè—ç»´åº¦ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”„ Transformer ç¼–ç å™¨å±‚\n",
    "\n",
    "```python\n",
    "(layers): ModuleList(\n",
    "  (0-31): 32 x LlamaDecoderLayer(...)\n",
    ")\n",
    "```\n",
    "\n",
    "* æ¨¡å‹æœ‰ 32 å±‚ï¼ˆè¡¨ç¤ºæ˜¯ **LLaMA-13B** æ¨¡å‹ï¼‰ã€‚\n",
    "* æ¯ä¸€å±‚æ˜¯ä¸€ä¸ª `LlamaDecoderLayer`ï¼Œç±»ä¼¼äºæ ‡å‡†çš„ Transformer è§£ç å™¨å±‚ï¼Œä½†ä¸º **è‡ªå›å½’ä»»åŠ¡** ç‰¹åˆ«å®šåˆ¶ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§  å•å±‚ç»“æ„ï¼šLlamaDecoderLayer\n",
    "\n",
    "ä»¥æ¯ä¸€å±‚ä¸ºä¾‹ï¼Œç»“æ„å¦‚ä¸‹ï¼š\n",
    "\n",
    "#### ğŸ”¸ æ³¨æ„åŠ›æœºåˆ¶ï¼ˆSelf-Attentionï¼‰\n",
    "\n",
    "```python\n",
    "(self_attn): LlamaAttention(\n",
    "  (q_proj): Linear4bit(4096 â†’ 4096)\n",
    "  (k_proj): Linear4bit(4096 â†’ 1024)\n",
    "  (v_proj): Linear4bit(4096 â†’ 1024)\n",
    "  (o_proj): Linear4bit(4096 â†’ 4096)\n",
    ")\n",
    "```\n",
    "\n",
    "* æ ‡å‡†å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼ˆMulti-head Self-Attentionï¼‰ã€‚\n",
    "* Qï¼ˆæŸ¥è¯¢ï¼‰ã€Kï¼ˆé”®ï¼‰ã€Vï¼ˆå€¼ï¼‰åˆ†åˆ«æ˜¯çº¿æ€§æŠ•å½±ã€‚\n",
    "* æ³¨æ„çš„æ˜¯ï¼š**è¿™é‡Œç”¨äº† 4-bit é‡åŒ–ï¼ˆLinear4bitï¼‰**ï¼Œå¤§å¹…å‡å°‘æ˜¾å­˜å’Œå†…å­˜å ç”¨ã€‚\n",
    "* `k_proj` å’Œ `v_proj` çš„è¾“å‡ºç»´åº¦æ˜¯ 1024ï¼Œæ„å‘³ç€å¯èƒ½ç”¨äº† 4 ä¸ªå¤´ï¼ˆ4096 Ã· 1024 = 4ï¼‰ã€‚\n",
    "\n",
    "#### ğŸ”¸ å‰é¦ˆç½‘ç»œï¼ˆMLPï¼‰\n",
    "\n",
    "```python\n",
    "(mlp): LlamaMLP(\n",
    "  (gate_proj): Linear4bit(4096 â†’ 14336)\n",
    "  (up_proj): Linear4bit(4096 â†’ 14336)\n",
    "  (down_proj): Linear4bit(14336 â†’ 4096)\n",
    "  (act_fn): SiLU()\n",
    ")\n",
    "```\n",
    "\n",
    "* å‰é¦ˆå±‚ç»“æ„é‡‡ç”¨ Gated Feedforwardï¼Œç±»ä¼¼äºï¼š\n",
    "\n",
    "  $$\n",
    "  \\text{FFN}(x) = \\text{SiLU}(x W_1) \\cdot (x W_2)\n",
    "  $$\n",
    "* `14336` æ˜¯ä¸­é—´ç»´åº¦ï¼ˆæ‰©å±•å±‚ï¼‰ï¼Œéå¸¸å®½ï¼Œå…¸å‹äº LLaMA æ¨¡å‹ã€‚\n",
    "* `SiLU` æ˜¯æ¿€æ´»å‡½æ•°ï¼Œä¹Ÿç§°ä¸º `Swish`ï¼Œé€‚åˆå¤§æ¨¡å‹è®­ç»ƒã€‚\n",
    "\n",
    "#### ğŸ”¸ å±‚å½’ä¸€åŒ–ï¼ˆLayerNormï¼‰\n",
    "\n",
    "```python\n",
    "(input_layernorm): LlamaRMSNorm((4096,), eps=1e-5)\n",
    "(post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-5)\n",
    "```\n",
    "\n",
    "* ä½¿ç”¨äº† **RMSNormï¼ˆå‡æ–¹æ ¹å½’ä¸€åŒ–ï¼‰** è€Œä¸æ˜¯ä¼ ç»Ÿçš„ LayerNormã€‚\n",
    "* RMSNorm æ›´è½»é‡ï¼Œä¸ä¾èµ–å‡å€¼ï¼Œåªä½¿ç”¨ L2 èŒƒæ•°è¿›è¡Œå½’ä¸€åŒ–ï¼Œé€‚åˆå¤§æ¨¡å‹åŠ é€Ÿã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ” æœ€ç»ˆå½’ä¸€åŒ–å’Œä½ç½®ç¼–ç \n",
    "\n",
    "```python\n",
    "(norm): LlamaRMSNorm((4096,), eps=1e-5)\n",
    "(rotary_emb): LlamaRotaryEmbedding()\n",
    "```\n",
    "\n",
    "* æ•´ä¸ª Transformer çš„æœ€åä¸€ä¸ªå±‚å½’ä¸€åŒ–ã€‚\n",
    "* `rotary_emb` è¡¨ç¤ºä½¿ç”¨ **æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰**ï¼Œæ›¿ä»£ç»å¯¹ä½ç½®ç¼–ç ï¼Œé€‚åˆæ”¯æŒæ— é™é•¿è¾“å…¥å’Œé«˜æ•ˆæ¨ç†ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ è¾“å‡ºå±‚ï¼ˆè¯­è¨€å»ºæ¨¡å¤´ï¼‰\n",
    "\n",
    "```python\n",
    "(lm_head): Linear(4096 â†’ 128256)\n",
    "```\n",
    "\n",
    "* å°† Transformer çš„è¾“å‡ºæ˜ å°„å›è¯è¡¨ç»´åº¦ï¼ˆé¢„æµ‹ä¸‹ä¸€ä¸ª token çš„æ¦‚ç‡åˆ†å¸ƒï¼‰ã€‚\n",
    "* æ²¡æœ‰ biasï¼ˆåç½®é¡¹ï¼‰ï¼Œè¿›ä¸€æ­¥å‡å°‘å‚æ•°ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¢ é‡åŒ–å±‚è¯´æ˜ï¼šLinear4bit\n",
    "\n",
    "* æ‰€æœ‰çš„çº¿æ€§å±‚éƒ½æ˜¯ `Linear4bit`ï¼Œæ¥è‡ª [bitsandbytes](https://github.com/TimDettmers/bitsandbytes)ã€‚\n",
    "* è¡¨ç¤ºè¿™éƒ¨åˆ†å‚æ•°æ˜¯ç”¨ 4-bit ç²¾åº¦å­˜å‚¨å’Œæ¨ç†çš„ï¼Œè€Œä¸æ˜¯ float16 æˆ– float32ã€‚\n",
    "* æå¤§å‡å°‘æ˜¾å­˜ä½¿ç”¨ï¼ˆé€šå¸¸æ˜¯ 3\\~4 å€ï¼‰ï¼Œä½¿å¾—åœ¨æ¶ˆè´¹çº§ GPUï¼ˆå¦‚ RTX 3090, 4090ï¼‰ä¸Šè¿è¡Œå¤§æ¨¡å‹æˆä¸ºå¯èƒ½ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… æ€»ç»“ï¼ˆæ ¸å¿ƒä¿¡æ¯ï¼‰\n",
    "\n",
    "| é¡¹ç›®       | å†…å®¹                      |\n",
    "| -------- | ----------------------- |\n",
    "| æ¨¡å‹ç±»å‹     | LLaMA è‡ªå›å½’è¯­è¨€æ¨¡å‹           |\n",
    "| å‚æ•°ç²¾åº¦     | 4-bit é‡åŒ–ï¼ˆbitsandbytesï¼‰  |\n",
    "| å±‚æ•°       | 32 å±‚                    |\n",
    "| éšè—å±‚ç»´åº¦    | 4096                    |\n",
    "| è¯è¡¨å¤§å°     | 128256                  |\n",
    "| MLP æ‰©å±•ç»´åº¦ | 14336                   |\n",
    "| æ¿€æ´»å‡½æ•°     | SiLUï¼ˆSwishï¼‰             |\n",
    "| å½’ä¸€åŒ–æ–¹æ³•    | RMSNorm                 |\n",
    "| ä½ç½®ç¼–ç      | Rotary Embedding (RoPE) |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f75fc0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
