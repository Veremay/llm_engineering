{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61f56afc-bc15-46a4-8eb1-d940c332cf52",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "And now - this colab unveils the heart (or the brains?) of the transformers library - the models:\n",
    "\n",
    "https://colab.research.google.com/drive/1hhR9Z-yiqjUe7pJjVQw4c74z_V3VchLy?usp=sharing\n",
    "\n",
    "This should run nicely on a low-cost or free T4 box."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbed898",
   "metadata": {},
   "source": [
    "my colab notebook:\n",
    "\n",
    "https://colab.research.google.com/drive/1-dUyawPg7d23opO_mBEwqsKoOEbKU-9j?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b15f53",
   "metadata": {},
   "source": [
    "## Some notes on the code:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff38dd8",
   "metadata": {},
   "source": [
    "```python\n",
    "from transformers import BitsAndBytesConfig\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                            # 开启 4-bit 加载\n",
    "    bnb_4bit_use_double_quant=True,               # 使用双重量化\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,        # 推理时使用 bfloat16\n",
    "    bnb_4bit_quant_type=\"nf4\"                     # 使用 NF4 量化类型\n",
    ")\n",
    "```\n",
    "`BitsAndBytesConfig` 是 `Transformers` 提供的一个类，用于配置和控制通过 `bitsandbytes` 库对模型进行量化加载的方式，常用于节省显存、加速推理。\n",
    "\n",
    "🔹`load_in_4bit=True`\n",
    "\n",
    "- 表示将模型加载为 4-bit 量化格式（而不是常见的 FP16、INT8 等），极大地减少显存使用（比如一个 13B 模型能在 24GB GPU 上运行）。\n",
    "\n",
    "- 需依赖 `bitsandbytes` 库。\n",
    "\n",
    "🔹 `bnb_4bit_use_double_quant=True`\n",
    "\n",
    "- 启用 双重量化（Double Quantization）：\n",
    "\n",
    "- 先将权重量化为更低精度，再对量化器本身也进行压缩。\n",
    "\n",
    "- 进一步减少模型体积，但会稍微增加计算复杂度。\n",
    "\n",
    "- 在多 GPU 部署下可显著节省内存。\n",
    "\n",
    "🔹 `bnb_4bit_compute_dtype=torch.bfloat16`\n",
    "\n",
    "- 指定模型推理时的计算精度。\n",
    "\n",
    "- `torch.bfloat16`（Brain Float 16）在新一代 GPU（如 A100、H100、4090）上具有良好支持，保留动态范围的同时节省显存。\n",
    "\n",
    "- 可替代 torch.float16（标准半精度），但对精度更友好。\n",
    "\n",
    "🔹 `bnb_4bit_quant_type=\"nf4\"`\n",
    "\n",
    "- 选择量化方法为 NF4（Normal Float 4）：\n",
    "\n",
    "- 是 `bitsandbytes` 提供的一种高级 4-bit 量化方法，相比传统 fp4 更准确。\n",
    "\n",
    "- 具有更好的保真度（对大模型效果影响更小）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa86756",
   "metadata": {},
   "source": [
    "这种设置非常适合：\n",
    "\n",
    "- 加载大语言模型（如 LLaMA-2、Mistral、Baichuan 等）到显存有限的 GPU（24GB 或更少）。\n",
    "\n",
    "- 节省内存同时保持较高推理性能。\n",
    "\n",
    "- 适合推理和轻量级微调（LoRA）任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3871cbb",
   "metadata": {},
   "source": [
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "def generate(model, messages):\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "  tokenizer.pad_token = tokenizer.eos_token\n",
    "  inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
    "  streamer = TextStreamer(tokenizer)\n",
    "  model = AutoModelForCausalLM.from_pretrained(model, device_map=\"auto\", quantization_config=quant_config)\n",
    "  outputs = model.generate(inputs, max_new_tokens=80, streamer=streamer)\n",
    "  del model, inputs, tokenizer, outputs, streamer\n",
    "  gc.collect()\n",
    "  torch.cuda.empty_cache()\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f76f68",
   "metadata": {},
   "source": [
    "🔹 加载分词器（Tokenizer）\n",
    "```python\n",
    "tokenizer = AutoTokenizer.from_pretrained(model) # 根据模型名自动加载相应的分词器。\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "```\n",
    "`tokenizer.pad_token = tokenizer.eos_token`：将 \\<pad\\> token 设置为 \\<eos\\>（结束符）。一些模型没有显式的 pad token，这样可以避免错误。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fca0846",
   "metadata": {},
   "source": [
    "🔹 应用聊天模板并转为张量\n",
    "```python\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    return_tensors=\"pt\",\n",
    "    add_generation_prompt=True\n",
    ").to(\"cuda\")\n",
    "```\n",
    "`apply_chat_template`：将对话 messages 转换为模型所需的格式（如 ChatML、Llama 2 的 prompt 格式等）。\n",
    "\n",
    "`return_tensors=\"pt\"`：将处理后的文本转换为 PyTorch 张量。\n",
    "\n",
    "`add_generation_prompt=True`：为生成添加一个“提示位”（让模型知道要继续生成回复）。\n",
    "\n",
    "`.to(\"cuda\")`：将输入数据移动到 GPU。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eada6e",
   "metadata": {},
   "source": [
    "🔹 创建流式输出器\n",
    "```python\n",
    "streamer = TextStreamer(tokenizer)\n",
    "```\n",
    "`TextStreamer` 是一个用于实时输出生成文本的工具，它会在模型生成的过程中实时打印结果。\n",
    "（通常是 Hugging Face 的 `transformers.TextStreamer`。）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa129a1",
   "metadata": {},
   "source": [
    "🔹 加载模型\n",
    "```python\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quant_config\n",
    ")\n",
    "```\n",
    "`AutoModelForCausalLM.from_pretrained(...)`：加载一个自回归语言模型（Causal LM）。\n",
    "\n",
    "`device_map=\"auto\"`：自动将模型分布到可用的 GPU 上。\n",
    "\n",
    "`quantization_config=quant_config`：使用量化配置加载模型（例如 4-bit、8-bit 以节省显存）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42650afc",
   "metadata": {},
   "source": [
    "🔹 文本生成\n",
    "```python\n",
    "outputs = model.generate(inputs, max_new_tokens=80, streamer=streamer)\n",
    "```\n",
    "`model.generate(...)`：生成最多 80 个新 token 的回复。\n",
    "\n",
    "`streamer=streamer`：边生成边打印。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d08406",
   "metadata": {},
   "source": [
    "🔹 清理内存\n",
    "```python\n",
    "del model, inputs, tokenizer, outputs, streamer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "```\n",
    "删除变量，释放显存。\n",
    "\n",
    "`gc.collect()`：触发 Python 的垃圾回收机制。\n",
    "\n",
    "`torch.cuda.empty_cache()`：清理 CUDA 缓存，防止显存占用持续上升。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90f220a",
   "metadata": {},
   "source": [
    "## 解释LlamaForCausalLM的结构"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab1a23d",
   "metadata": {},
   "source": [
    "这段模型结构是一个 **LLaMA 大语言模型（LLM）用于自回归语言建模（Causal LM）** 的精简表示，模型为 4-bit 量化版本。我们逐层解释这份架构：\n",
    "\n",
    "---\n",
    "\n",
    "### 🔧 顶层模型\n",
    "\n",
    "```python\n",
    "LlamaForCausalLM(\n",
    "  (model): LlamaModel(...)\n",
    "  (lm_head): Linear(...)\n",
    ")\n",
    "```\n",
    "\n",
    "这是 Hugging Face `transformers` 中的标准封装：\n",
    "\n",
    "* `LlamaForCausalLM`：用于语言建模（如生成、补全），继承自 `PreTrainedModel`。\n",
    "* `model`: 实际的 Transformer 主体（`LlamaModel`）。\n",
    "* `lm_head`: 语言建模头部（将隐藏层映射到词表的线性层），用于预测下一个 token。\n",
    "\n",
    "---\n",
    "\n",
    "### 🧱 嵌入层\n",
    "\n",
    "```python\n",
    "(embed_tokens): Embedding(128256, 4096)\n",
    "```\n",
    "\n",
    "* `Embedding(vocab_size=128256, hidden_size=4096)`：\n",
    "\n",
    "  * 将输入 token ID 映射为维度为 4096 的向量。\n",
    "  * `128256` 是词表大小。\n",
    "  * `4096` 是模型的隐藏维度。\n",
    "\n",
    "---\n",
    "\n",
    "### 🔄 Transformer 编码器层\n",
    "\n",
    "```python\n",
    "(layers): ModuleList(\n",
    "  (0-31): 32 x LlamaDecoderLayer(...)\n",
    ")\n",
    "```\n",
    "\n",
    "* 模型有 32 层（表示是 **LLaMA-13B** 模型）。\n",
    "* 每一层是一个 `LlamaDecoderLayer`，类似于标准的 Transformer 解码器层，但为 **自回归任务** 特别定制。\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 单层结构：LlamaDecoderLayer\n",
    "\n",
    "以每一层为例，结构如下：\n",
    "\n",
    "#### 🔸 注意力机制（Self-Attention）\n",
    "\n",
    "```python\n",
    "(self_attn): LlamaAttention(\n",
    "  (q_proj): Linear4bit(4096 → 4096)\n",
    "  (k_proj): Linear4bit(4096 → 1024)\n",
    "  (v_proj): Linear4bit(4096 → 1024)\n",
    "  (o_proj): Linear4bit(4096 → 4096)\n",
    ")\n",
    "```\n",
    "\n",
    "* 标准多头注意力机制（Multi-head Self-Attention）。\n",
    "* Q（查询）、K（键）、V（值）分别是线性投影。\n",
    "* 注意的是：**这里用了 4-bit 量化（Linear4bit）**，大幅减少显存和内存占用。\n",
    "* `k_proj` 和 `v_proj` 的输出维度是 1024，意味着可能用了 4 个头（4096 ÷ 1024 = 4）。\n",
    "\n",
    "#### 🔸 前馈网络（MLP）\n",
    "\n",
    "```python\n",
    "(mlp): LlamaMLP(\n",
    "  (gate_proj): Linear4bit(4096 → 14336)\n",
    "  (up_proj): Linear4bit(4096 → 14336)\n",
    "  (down_proj): Linear4bit(14336 → 4096)\n",
    "  (act_fn): SiLU()\n",
    ")\n",
    "```\n",
    "\n",
    "* 前馈层结构采用 Gated Feedforward，类似于：\n",
    "\n",
    "  $$\n",
    "  \\text{FFN}(x) = \\text{SiLU}(x W_1) \\cdot (x W_2)\n",
    "  $$\n",
    "* `14336` 是中间维度（扩展层），非常宽，典型于 LLaMA 模型。\n",
    "* `SiLU` 是激活函数，也称为 `Swish`，适合大模型训练。\n",
    "\n",
    "#### 🔸 层归一化（LayerNorm）\n",
    "\n",
    "```python\n",
    "(input_layernorm): LlamaRMSNorm((4096,), eps=1e-5)\n",
    "(post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-5)\n",
    "```\n",
    "\n",
    "* 使用了 **RMSNorm（均方根归一化）** 而不是传统的 LayerNorm。\n",
    "* RMSNorm 更轻量，不依赖均值，只使用 L2 范数进行归一化，适合大模型加速。\n",
    "\n",
    "---\n",
    "\n",
    "### 🔁 最终归一化和位置编码\n",
    "\n",
    "```python\n",
    "(norm): LlamaRMSNorm((4096,), eps=1e-5)\n",
    "(rotary_emb): LlamaRotaryEmbedding()\n",
    "```\n",
    "\n",
    "* 整个 Transformer 的最后一个层归一化。\n",
    "* `rotary_emb` 表示使用 **旋转位置编码（RoPE）**，替代绝对位置编码，适合支持无限长输入和高效推理。\n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 输出层（语言建模头）\n",
    "\n",
    "```python\n",
    "(lm_head): Linear(4096 → 128256)\n",
    "```\n",
    "\n",
    "* 将 Transformer 的输出映射回词表维度（预测下一个 token 的概率分布）。\n",
    "* 没有 bias（偏置项），进一步减少参数。\n",
    "\n",
    "---\n",
    "\n",
    "### 🔢 量化层说明：Linear4bit\n",
    "\n",
    "* 所有的线性层都是 `Linear4bit`，来自 [bitsandbytes](https://github.com/TimDettmers/bitsandbytes)。\n",
    "* 表示这部分参数是用 4-bit 精度存储和推理的，而不是 float16 或 float32。\n",
    "* 极大减少显存使用（通常是 3\\~4 倍），使得在消费级 GPU（如 RTX 3090, 4090）上运行大模型成为可能。\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ 总结（核心信息）\n",
    "\n",
    "| 项目       | 内容                      |\n",
    "| -------- | ----------------------- |\n",
    "| 模型类型     | LLaMA 自回归语言模型           |\n",
    "| 参数精度     | 4-bit 量化（bitsandbytes）  |\n",
    "| 层数       | 32 层                    |\n",
    "| 隐藏层维度    | 4096                    |\n",
    "| 词表大小     | 128256                  |\n",
    "| MLP 扩展维度 | 14336                   |\n",
    "| 激活函数     | SiLU（Swish）             |\n",
    "| 归一化方法    | RMSNorm                 |\n",
    "| 位置编码     | Rotary Embedding (RoPE) |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f75fc0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
