{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Chinchilla Scaling Law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of parameters ~ proportional to the number of training tokens**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7 common benchmarks**\n",
    "\n",
    "| Benchmark | What's being evaluated | Description |\n",
    "|-----------|----------------------|-------------|\n",
    "| ARC | Reasoning | A benchmark for evaluating scientific reasoning; multiple-choice questions |\n",
    "| DROP | Language Comp | Distill details from text then add, count or sort |\n",
    "| HellaSwag | Common Sense | \"Harder Endings, Long Contexts and Low Shot Activities\" |\n",
    "| MMLU | Understanding | Factual recall, reasoning and problem solving across 57 subjects |\n",
    "| TruthfulQA | Accuracy | Robustness in providing truthful replies in adversarial conditions |\n",
    "| Winogrande | Context | Test the LLM understands context and resolves ambiguity |\n",
    "| GSM8K | Math | Math and word problems taught in elementary and middle schools |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3 specific benchmarks**\n",
    "\n",
    "| Benchmark | What's being evaluated | Description |\n",
    "|-----------|----------------------|-------------|\n",
    "| ELO | Chat | Results from head-to-head face-offs with other LLMs, as with ELO in Chess |\n",
    "| HumanEval | Python Coding | 164 problems writing code based on docstrings |\n",
    "| MultiPL-E | Broader Coding | Translation of HumanEval to 18 programming languages |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limitations of the benchmarks**\n",
    "\n",
    "- Not consistently applied\n",
    "- Too narrow in scope\n",
    "- Hard to measure nuanced reasoning\n",
    "- Training data leakage\n",
    "- Overfitting\n",
    "\n",
    "And a new concern, not yet proven\n",
    "\n",
    "- Frontier LLMs may be aware that they are beingevaluated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6 hard, next-level benchmarks**\n",
    "\n",
    "| Benchmark | What's being evaluated | Description |\n",
    "|-----------|----------------------|-------------|\n",
    "| GPQA | Graduate Tests | 448 expert questions; non-PhD humans score 34% even with web access |\n",
    "| BBHard | Future Capabilities | 204 tasks believed beyond capabilities of LLMs (no longer!) |\n",
    "| Math Lv 5 | Math | High-school level math competition problems |\n",
    "| IFEval | Difficult instructions | Like, \"write more than 400 words\" and \"mention AI at least 3 times\" |\n",
    "| MuSR | Multistep Soft Reasoning | Logical deduction, such as analyzing 1,000 word murder mystery and answering: \"Who has means, motive and opportunity?\" |\n",
    "| MMLU-PRO | Harder MMLU | A more advanced and cleaned up version of MMLU including choice of 10 answers instead of 4 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huggingface OpenLLM Leaderboard\n",
    "\n",
    "https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/\n",
    "\n",
    "https://huggingface.co/spaces/open-llm-leaderboard-old/open_llm_leaderboard\n",
    "\n",
    "https://huggingface.co/spaces/open-llm-leaderboard/comparator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
